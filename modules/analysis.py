"""
äºšé©¬é€Šè¯„è®ºæƒ…æ„Ÿåˆ†æç³»ç»Ÿ - ä¿®å¤ç‰ˆ
ä¿®å¤ä¸­æ€§è¯„è®ºè¯†åˆ«å’Œå¯è§†åŒ–é‡å é—®é¢˜
"""

import os
import json
import pandas as pd
import numpy as np
from openai import OpenAI
import time
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import jieba
import re
from datetime import datetime
import warnings

warnings.filterwarnings('ignore')

# ==================== é…ç½®åŒºåŸŸ ====================
# è®¾ç½®å­—ä½“ï¼Œé¿å…æ˜¾ç¤ºé—®é¢˜
plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei']
plt.rcParams['axes.unicode_minus'] = False

# APIé…ç½®
USE_API = False  # True: ä½¿ç”¨ DeepSeek API, False: ä½¿ç”¨æœ¬åœ°åˆ†æ
DEEPSEEK_API_KEY = "sk-a7dbc3801c44491c9081462edd4104fa"


class EnhancedSentimentAnalyzer:
    """å¢å¼ºç‰ˆæƒ…æ„Ÿåˆ†æå™¨ - ç®€åŒ–ç‰ˆ"""

    def __init__(self):
        """åˆå§‹åŒ–æƒ…æ„Ÿè¯å…¸"""
        # æ­£é¢è¯æ±‡
        self.positive_words = set([
            'excellent', 'perfect', 'amazing', 'fantastic', 'wonderful', 'awesome',
            'outstanding', 'superb', 'exceptional', 'brilliant', 'magnificent',
            'great', 'good', 'nice', 'well', 'better', 'best', 'quality',
            'worth', 'value', 'works', 'working', 'functional', 'effective',
            'fast', 'easy', 'simple', 'convenient', 'comfortable', 'useful',
            'love', 'loved', 'like', 'liked', 'enjoy', 'satisfied', 'pleased',
            'happy', 'recommend', 'recommended', 'beautiful', 'pretty'
        ])

        # è´Ÿé¢è¯æ±‡
        self.negative_words = set([
            'terrible', 'horrible', 'awful', 'worst', 'disgusting', 'hate',
            'bad', 'poor', 'inferior', 'weak', 'wrong', 'useless', 'worthless',
            'broken', 'damaged', 'defective', 'faulty', 'cheap', 'junk',
            'failed', 'failure', 'fail', 'malfunction', 'unreliable', 'slow',
            'disappointed', 'disappointing', 'dissatisfied', 'unhappy', 'frustrated',
            'waste', 'wasted', 'fake', 'scam', 'return', 'returned', 'refund'
        ])

        # ä¸­æ€§è¯æ±‡
        self.neutral_words = set([
            'okay', 'ok', 'fine', 'average', 'normal', 'standard', 'acceptable',
            'adequate', 'fair', 'moderate', 'reasonable', 'decent', 'satisfactory',
            'alright', 'so-so', 'ordinary', 'typical', 'regular', 'expected'
        ])

        # å¦å®šè¯
        self.negation_words = set([
            'not', 'no', 'never', 'neither', 'nor', 'cannot', "can't",
            "won't", "doesn't", "didn't", "isn't", "aren't", "wasn't"
        ])

    def analyze(self, text, rating=None):
        """
        åˆ†æå•æ¡è¯„è®ºçš„æƒ…æ„Ÿ - ç®€åŒ–ç‰ˆ

        Args:
            text: è¯„è®ºæ–‡æœ¬
            rating: æ˜Ÿçº§è¯„åˆ† (1-5)
        """
        if pd.isna(text) or str(text).strip() == '':
            return 'neutral', 0.5

        text = str(text).lower()
        words = text.split()

        # åˆå§‹åŒ–åˆ†æ•°
        positive_score = 0
        negative_score = 0
        neutral_score = 0

        # åŸºäºè¯„åˆ†çš„åˆå§‹åå‘
        if rating is not None and not pd.isna(rating):
            rating = float(rating)
            if rating <= 2:
                negative_score += 3
            elif rating == 3:
                neutral_score += 5  # 3æ˜Ÿå¼ºçƒˆæš—ç¤ºä¸­æ€§
            elif rating >= 4:
                positive_score += 2

        # ç»Ÿè®¡æƒ…æ„Ÿè¯
        for word in words:
            word_clean = re.sub(r'[^\w]', '', word)
            if word_clean in self.positive_words:
                positive_score += 1
            elif word_clean in self.negative_words:
                negative_score += 1
            elif word_clean in self.neutral_words:
                neutral_score += 1

        # æ£€æŸ¥ä¸­æ€§çŸ­è¯­
        neutral_phrases = ['not bad', 'not great', 'just okay', 'nothing special',
                           'as expected', 'its okay', 'its fine', 'could be better']

        text_lower = text.lower()
        for phrase in neutral_phrases:
            if phrase in text_lower:
                neutral_score += 2

        # ç‰¹æ®ŠçŸ­è¯­æ£€æµ‹
        if 'highly recommend' in text_lower or 'definitely recommend' in text_lower:
            positive_score += 3
        if 'do not recommend' in text_lower or "don't recommend" in text_lower:
            negative_score += 3
        if 'waste of money' in text_lower or 'terrible quality' in text_lower:
            negative_score += 2

        # å†³å®šæƒ…æ„Ÿ
        total_score = positive_score + negative_score + neutral_score

        # å¦‚æœæ²¡æœ‰æ˜æ˜¾æƒ…æ„Ÿè¯
        if total_score == 0:
            if rating == 3:
                return 'neutral', 0.7
            elif rating <= 2:
                return 'negative', 0.6
            elif rating >= 4:
                return 'positive', 0.6
            else:
                return 'neutral', 0.5

        # è®¡ç®—å„æƒ…æ„Ÿæ¯”ä¾‹
        pos_ratio = positive_score / total_score
        neg_ratio = negative_score / total_score
        neu_ratio = neutral_score / total_score

        # åˆ¤æ–­é€»è¾‘ - æ›´åŠ å¹³è¡¡
        # 1. å¦‚æœä¸­æ€§åˆ†æ•°æœ€é«˜æˆ–è€…3æ˜Ÿè¯„è®º
        if rating == 3 and neutral_score >= max(positive_score, negative_score) * 0.5:
            return 'neutral', 0.7 + neu_ratio * 0.2

        # 2. å¦‚æœä¸­æ€§è¯å ä¸»å¯¼
        if neu_ratio > 0.4 or neutral_score > max(positive_score, negative_score):
            return 'neutral', 0.6 + neu_ratio * 0.3

        # 3. å¦‚æœæ­£è´Ÿé¢æ¥è¿‘ï¼ˆæ··åˆæƒ…æ„Ÿï¼‰
        if positive_score > 0 and negative_score > 0:
            diff = abs(positive_score - negative_score)
            if diff <= 2:
                return 'neutral', 0.65

        # 4. åŸºäºè¯„åˆ†çš„åˆ¤æ–­
        if rating is not None:
            if rating <= 2:
                if negative_score >= positive_score:
                    return 'negative', 0.7 + neg_ratio * 0.2
                elif positive_score > negative_score * 2:
                    return 'neutral', 0.6  # è¯„åˆ†ä½ä½†æ–‡æœ¬æ­£é¢ï¼Œå¯èƒ½æ˜¯ä¸­æ€§
                else:
                    return 'neutral', 0.55
            elif rating >= 4:
                if positive_score >= negative_score:
                    return 'positive', 0.7 + pos_ratio * 0.2
                elif negative_score > positive_score * 2:
                    return 'neutral', 0.6  # è¯„åˆ†é«˜ä½†æ–‡æœ¬è´Ÿé¢ï¼Œå¯èƒ½æ˜¯ä¸­æ€§
                else:
                    return 'neutral', 0.55

        # 5. åŸºäºæ–‡æœ¬çš„æœ€ç»ˆåˆ¤æ–­
        if positive_score > negative_score + 2:
            return 'positive', 0.6 + pos_ratio * 0.3
        elif negative_score > positive_score + 2:
            return 'negative', 0.6 + neg_ratio * 0.3
        else:
            return 'neutral', 0.6


class AmazonReviewAnalyzer:
    """äºšé©¬é€Šè¯„è®ºåˆ†æä¸»ç±»"""

    def __init__(self, use_api=False, api_key=None):
        """åˆå§‹åŒ–åˆ†æå™¨"""
        self.use_api = use_api
        self.api_key = api_key
        self.client = None
        self.local_analyzer = EnhancedSentimentAnalyzer()

        if self.use_api and self.api_key:
            try:
                self.client = OpenAI(
                    api_key=self.api_key,
                    base_url="https://api.deepseek.com"
                )
                print("âœ… DeepSeek API è¿æ¥æˆåŠŸ")
            except Exception as e:
                print(f"âŒ DeepSeek API è¿æ¥å¤±è´¥: {e}")
                print("âš ï¸ è‡ªåŠ¨åˆ‡æ¢åˆ°æœ¬åœ°åˆ†ææ¨¡å¼")
                self.use_api = False

    def analyze_sentiment_api(self, text, rating=None):
        """ä½¿ç”¨APIåˆ†ææƒ…æ„Ÿ"""
        if not self.client:
            return self.analyze_sentiment_local(text, rating)

        try:
            text_truncated = text[:500] if len(text) > 500 else text
            prompt = f"Analyze this Amazon review sentiment"
            if rating is not None:
                prompt += f" (Rating: {rating}/5 stars)"
            prompt += f": {text_truncated}"

            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {
                        "role": "system",
                        "content": "Analyze sentiment. Return ONLY: positive, negative, or neutral. Consider rating: 3 stars often means neutral."
                    },
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=10,
                timeout=10
            )

            result = response.choices[0].message.content.strip().lower()
            if 'positive' in result:
                return 'positive', 0.9
            elif 'negative' in result:
                return 'negative', 0.9
            else:
                return 'neutral', 0.7

        except Exception as e:
            return self.analyze_sentiment_local(text, rating)

    def analyze_sentiment_local(self, text, rating=None):
        """ä½¿ç”¨æœ¬åœ°åˆ†æå™¨åˆ†ææƒ…æ„Ÿ"""
        return self.local_analyzer.analyze(text, rating)

    def analyze_sentiment(self, text, rating=None):
        """ç»Ÿä¸€çš„æƒ…æ„Ÿåˆ†ææ¥å£"""
        if self.use_api and self.client:
            return self.analyze_sentiment_api(text, rating)
        else:
            return self.analyze_sentiment_local(text, rating)

    def process_file(self, file_path):
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        reviews = []

        if file_path.endswith('.csv'):
            df = pd.read_csv(file_path, encoding='utf-8')
            review_cols = ['review', 'body', 'text', 'comment', 'content']
            title_cols = ['title', 'subject', 'heading']

            review_col = None
            title_col = None

            for col in review_cols:
                if col in df.columns:
                    review_col = col
                    break

            for col in title_cols:
                if col in df.columns:
                    title_col = col
                    break

            if review_col:
                for idx, row in df.iterrows():
                    content = str(row[review_col]) if pd.notna(row[review_col]) else ''
                    if title_col and pd.notna(row[title_col]):
                        content = str(row[title_col]) + ' ' + content

                    if content.strip():
                        reviews.append({
                            'filename': os.path.basename(file_path),
                            'content': content,
                            'rating': row.get('rating', None),
                            'username': row.get('username', 'Unknown'),
                            'product_id': row.get('product_id', 'Unknown'),
                            'product_title': row.get('product_title', 'Unknown Product')
                        })

        return reviews

    def analyze_reviews(self, input_source='reviews'):
        """åˆ†æè¯„è®º"""
        reviews = []

        if os.path.isfile(input_source):
            print(f"ğŸ“– è¯»å–æ–‡ä»¶: {input_source}")
            reviews = self.process_file(input_source)
        else:
            print(f"âŒ æ‰¾ä¸åˆ°è¾“å…¥æº: {input_source}")
            return None

        if not reviews:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„è¯„è®ºæ•°æ®")
            return None

        print(f"âœ… æˆåŠŸè¯»å– {len(reviews)} æ¡è¯„è®º")

        # ç»Ÿè®¡è¯„åˆ†åˆ†å¸ƒ
        ratings = [r.get('rating') for r in reviews if r.get('rating') is not None]
        if ratings:
            print(f"\nğŸ“Š è¯„åˆ†åˆ†å¸ƒ:")
            for star in range(1, 6):
                count = sum(1 for r in ratings if r == star)
                if count > 0:
                    print(f"   {star}æ˜Ÿ: {count} æ¡ ({count / len(ratings) * 100:.1f}%)")

        # åˆ†ææƒ…æ„Ÿ
        print(f"\nğŸ” å¼€å§‹æƒ…æ„Ÿåˆ†æ...")
        print(f"   æ¨¡å¼: {'ğŸš€ DeepSeek API' if (self.use_api and self.client) else 'ğŸ’» æœ¬åœ°åˆ†æ'}")

        results = []
        total = len(reviews)
        sentiment_counter = {'positive': 0, 'negative': 0, 'neutral': 0}

        for idx, review in enumerate(reviews):
            sentiment, confidence = self.analyze_sentiment(
                review['content'],
                review.get('rating')
            )

            sentiment_counter[sentiment] += 1

            result = {
                'filename': review['filename'],
                'content': review['content'][:200] + '...' if len(review['content']) > 200 else review['content'],
                'full_content': review['content'],
                'sentiment': sentiment,
                'confidence': confidence,
                'rating': review.get('rating'),
                'username': review.get('username', 'Unknown'),
                'product_id': review.get('product_id', 'Unknown'),
                'product_title': review.get('product_title', 'Unknown Product')
            }
            results.append(result)

            if (idx + 1) % 10 == 0 or (idx + 1) == total:
                progress = (idx + 1) / total * 100
                print(f"   è¿›åº¦: {idx + 1}/{total} ({progress:.1f}%) - "
                      f"æ­£é¢: {sentiment_counter['positive']}, "
                      f"è´Ÿé¢: {sentiment_counter['negative']}, "
                      f"ä¸­æ€§: {sentiment_counter['neutral']}")

            if self.use_api and self.client and idx < total - 1:
                time.sleep(0.1)

        print("âœ… åˆ†æå®Œæˆï¼")

        print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   æ­£é¢è¯„è®º: {sentiment_counter['positive']} ({sentiment_counter['positive'] / total * 100:.1f}%)")
        print(f"   è´Ÿé¢è¯„è®º: {sentiment_counter['negative']} ({sentiment_counter['negative'] / total * 100:.1f}%)")
        print(f"   ä¸­æ€§è¯„è®º: {sentiment_counter['neutral']} ({sentiment_counter['neutral'] / total * 100:.1f}%)")

        df = pd.DataFrame(results)
        return df

    def generate_report(self, df):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if df is None or len(df) == 0:
            print("âŒ æ²¡æœ‰æ•°æ®å¯ä»¥ç”ŸæˆæŠ¥å‘Š")
            return

        print("\nğŸ“Š ç”Ÿæˆåˆ†ææŠ¥å‘Š...")

        if not os.path.exists('analysis_results'):
            os.makedirs('analysis_results')

        total = len(df)
        sentiment_counts = df['sentiment'].value_counts()

        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append(" " * 25 + "Amazon Review Sentiment Analysis Report")
        report_lines.append("=" * 80)
        report_lines.append(f"Generation Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"Total Reviews: {total}")
        report_lines.append("")

        # 1. æƒ…æ„Ÿåˆ†å¸ƒ
        report_lines.append("[Sentiment Distribution]")
        report_lines.append("-" * 40)

        for sentiment in ['positive', 'negative', 'neutral']:
            if sentiment in sentiment_counts.index:
                count = sentiment_counts[sentiment]
                percentage = count / total * 100
                emoji = {'positive': 'ğŸ˜Š', 'negative': 'ğŸ˜”', 'neutral': 'ğŸ˜'}[sentiment]
                bar = 'â–ˆ' * int(percentage / 2)
                report_lines.append(
                    f"{emoji} {sentiment.capitalize():8s}: {count:4d} reviews ({percentage:6.2f}%) {bar}")

        # 2. æ˜Ÿçº§ç»Ÿè®¡ï¼ˆå¦‚æœæœ‰è¯„åˆ†æ•°æ®ï¼‰
        if 'rating' in df.columns and df['rating'].notna().any():
            report_lines.append("")
            report_lines.append("[Rating Statistics]")
            report_lines.append("-" * 40)
            report_lines.append(f"Average Rating: {df['rating'].mean():.2f} / 5.0")
            report_lines.append(f"Median Rating: {df['rating'].median():.1f}")
            report_lines.append(f"Standard Deviation: {df['rating'].std():.2f}")

            # è¯„åˆ†åˆ†å¸ƒè¯¦ç»†ç»Ÿè®¡
            report_lines.append("")
            report_lines.append("Rating Distribution:")
            for star in range(5, 0, -1):  # ä»5æ˜Ÿåˆ°1æ˜Ÿ
                star_count = len(df[df['rating'] == star])
                if star_count > 0:
                    star_percentage = star_count / total * 100
                    stars_display = 'â˜…' * star + 'â˜†' * (5 - star)
                    bar = 'â–ˆ' * int(star_percentage / 2)
                    report_lines.append(f"  {stars_display}: {star_count:4d} ({star_percentage:5.1f}%) {bar}")

            # æ˜Ÿçº§ä¸æƒ…æ„Ÿçš„äº¤å‰åˆ†æ
            report_lines.append("")
            report_lines.append("[Rating-Sentiment Cross Analysis]")
            report_lines.append("-" * 40)

            for star in range(5, 0, -1):
                star_df = df[df['rating'] == star]
                if len(star_df) > 0:
                    star_sentiments = star_df['sentiment'].value_counts()
                    report_lines.append(f"{star}â˜… Reviews ({len(star_df)} total):")
                    for sentiment in ['positive', 'negative', 'neutral']:
                        if sentiment in star_sentiments.index:
                            count = star_sentiments[sentiment]
                            pct = count / len(star_df) * 100
                            report_lines.append(f"    {sentiment:8s}: {count:3d} ({pct:5.1f}%)")

        # 3. é«˜é¢‘è¯åˆ†æ
        report_lines.append("")
        report_lines.append("[High Frequency Words Analysis]")
        report_lines.append("-" * 40)

        # åœç”¨è¯åˆ—è¡¨
        stop_words = {
            'the', 'and', 'for', 'with', 'this', 'that', 'have', 'from',
            'will', 'your', 'more', 'been', 'what', 'were', 'there', 'their',
            'would', 'could', 'very', 'been', 'have', 'also', 'just', 'only',
            'other', 'after', 'before', 'some', 'when', 'which', 'where',
            'these', 'those', 'then', 'than', 'both', 'each', 'they', 'them',
            'was', 'are', 'has', 'had', 'but', 'not', 'can', 'did', 'does'
        }

        # åˆ†ææ•´ä½“é«˜é¢‘è¯
        all_text = ' '.join(df['full_content'].astype(str))
        all_words = re.findall(r'\b[a-zA-Z]{3,}\b', all_text.lower())
        all_filtered = [w for w in all_words if w not in stop_words]
        all_freq = Counter(all_filtered).most_common(15)

        report_lines.append("Overall Top 15 Words:")
        for i, (word, count) in enumerate(all_freq, 1):
            report_lines.append(f"  {i:2d}. {word:15s} ({count:4d} times)")

        # åˆ†åˆ«åˆ†æå„æƒ…æ„Ÿç±»åˆ«çš„é«˜é¢‘è¯
        report_lines.append("")
        report_lines.append("Top Words by Sentiment:")

        for sentiment in ['positive', 'negative', 'neutral']:
            sentiment_reviews = df[df['sentiment'] == sentiment]['full_content'].astype(str)
            if len(sentiment_reviews) > 0:
                sentiment_text = ' '.join(sentiment_reviews)
                sentiment_words = re.findall(r'\b[a-zA-Z]{3,}\b', sentiment_text.lower())
                sentiment_filtered = [w for w in sentiment_words if w not in stop_words]
                sentiment_freq = Counter(sentiment_filtered).most_common(10)

                emoji = {'positive': 'ğŸ˜Š', 'negative': 'ğŸ˜”', 'neutral': 'ğŸ˜'}[sentiment]
                report_lines.append(f"\n{emoji} {sentiment.capitalize()} Reviews:")
                for i, (word, count) in enumerate(sentiment_freq[:5], 1):
                    report_lines.append(f"    {i}. {word:15s} ({count:3d} times)")

        # 4. å…³é”®çŸ­è¯­åˆ†æ
        report_lines.append("")
        report_lines.append("[Key Phrases Analysis]")
        report_lines.append("-" * 40)

        # æå–2-3è¯çš„çŸ­è¯­
        def extract_phrases(text, n=2):
            words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
            phrases = []
            for i in range(len(words) - n + 1):
                phrase_words = words[i:i + n]
                if not any(w in stop_words for w in phrase_words):
                    phrases.append(' '.join(phrase_words))
            return phrases

        # åˆ†ææ­£é¢è¯„è®ºçš„å…³é”®çŸ­è¯­
        positive_text = ' '.join(df[df['sentiment'] == 'positive']['full_content'].astype(str))
        if positive_text:
            pos_phrases = extract_phrases(positive_text, 2)
            pos_phrase_freq = Counter(pos_phrases).most_common(5)
            report_lines.append("Positive Review Key Phrases:")
            for phrase, count in pos_phrase_freq:
                if count > 2:  # åªæ˜¾ç¤ºå‡ºç°3æ¬¡ä»¥ä¸Šçš„çŸ­è¯­
                    report_lines.append(f"  â€¢ \"{phrase}\" ({count} times)")

        # åˆ†æè´Ÿé¢è¯„è®ºçš„å…³é”®çŸ­è¯­
        negative_text = ' '.join(df[df['sentiment'] == 'negative']['full_content'].astype(str))
        if negative_text:
            neg_phrases = extract_phrases(negative_text, 2)
            neg_phrase_freq = Counter(neg_phrases).most_common(5)
            report_lines.append("\nNegative Review Key Phrases:")
            for phrase, count in neg_phrase_freq:
                if count > 2:
                    report_lines.append(f"  â€¢ \"{phrase}\" ({count} times)")

        # 5. ç½®ä¿¡åº¦ç»Ÿè®¡
        report_lines.append("")
        report_lines.append("[Analysis Confidence Statistics]")
        report_lines.append("-" * 40)
        report_lines.append(f"Average Confidence: {df['confidence'].mean():.2%}")
        report_lines.append(
            f"High Confidence (>80%): {len(df[df['confidence'] > 0.8]):4d} reviews ({len(df[df['confidence'] > 0.8]) / total * 100:.1f}%)")
        report_lines.append(
            f"Medium Confidence (50-80%): {len(df[(df['confidence'] >= 0.5) & (df['confidence'] <= 0.8)]):4d} reviews ({len(df[(df['confidence'] >= 0.5) & (df['confidence'] <= 0.8)]) / total * 100:.1f}%)")
        report_lines.append(
            f"Low Confidence (<50%): {len(df[df['confidence'] < 0.5]):4d} reviews ({len(df[df['confidence'] < 0.5]) / total * 100:.1f}%)")

        # 6. å…¸å‹è¯„è®ºç¤ºä¾‹
        report_lines.append("")
        report_lines.append("[Representative Review Examples]")
        report_lines.append("-" * 40)

        # æœ€æ­£é¢çš„è¯„è®º
        if len(df[df['sentiment'] == 'positive']) > 0:
            best_review = df[df['sentiment'] == 'positive'].nlargest(1, 'confidence').iloc[0]
            report_lines.append("Most Confident Positive Review:")
            if pd.notna(best_review['rating']):
                report_lines.append(
                    f"  Rating: {'â˜…' * int(best_review['rating']) + 'â˜†' * (5 - int(best_review['rating']))}")
            report_lines.append(f"  Confidence: {best_review['confidence']:.2%}")
            report_lines.append(f"  Content: \"{best_review['content'][:200]}...\"" if len(
                best_review['content']) > 200 else f"  Content: \"{best_review['content']}\"")

        # æœ€è´Ÿé¢çš„è¯„è®º
        if len(df[df['sentiment'] == 'negative']) > 0:
            worst_review = df[df['sentiment'] == 'negative'].nlargest(1, 'confidence').iloc[0]
            report_lines.append("")
            report_lines.append("Most Confident Negative Review:")
            if pd.notna(worst_review['rating']):
                report_lines.append(
                    f"  Rating: {'â˜…' * int(worst_review['rating']) + 'â˜†' * (5 - int(worst_review['rating']))}")
            report_lines.append(f"  Confidence: {worst_review['confidence']:.2%}")
            report_lines.append(f"  Content: \"{worst_review['content'][:200]}...\"" if len(
                worst_review['content']) > 200 else f"  Content: \"{worst_review['content']}\"")

        # å…¸å‹ä¸­æ€§è¯„è®º
        if len(df[df['sentiment'] == 'neutral']) > 0:
            neutral_review = df[df['sentiment'] == 'neutral'].nlargest(1, 'confidence').iloc[0]
            report_lines.append("")
            report_lines.append("Most Confident Neutral Review:")
            if pd.notna(neutral_review['rating']):
                report_lines.append(
                    f"  Rating: {'â˜…' * int(neutral_review['rating']) + 'â˜†' * (5 - int(neutral_review['rating']))}")
            report_lines.append(f"  Confidence: {neutral_review['confidence']:.2%}")
            report_lines.append(f"  Content: \"{neutral_review['content'][:200]}...\"" if len(
                neutral_review['content']) > 200 else f"  Content: \"{neutral_review['content']}\"")

        # 7. åˆ†ææ´å¯Ÿ
        report_lines.append("")
        report_lines.append("[Analysis Insights]")
        report_lines.append("-" * 40)

        # è®¡ç®—å„ç§æŒ‡æ ‡
        pos_ratio = sentiment_counts.get('positive', 0) / total * 100
        neg_ratio = sentiment_counts.get('negative', 0) / total * 100
        neu_ratio = sentiment_counts.get('neutral', 0) / total * 100

        if 'rating' in df.columns and df['rating'].notna().any():
            avg_rating = df['rating'].mean()

            # è¯„åˆ†ä¸æƒ…æ„Ÿä¸€è‡´æ€§åˆ†æ
            low_rating_positive = len(df[(df['rating'] <= 2) & (df['sentiment'] == 'positive')])
            high_rating_negative = len(df[(df['rating'] >= 4) & (df['sentiment'] == 'negative')])
            consistency_issues = low_rating_positive + high_rating_negative

            if consistency_issues > 0:
                report_lines.append(f"âš ï¸ Found {consistency_issues} reviews with sentiment-rating mismatch")
                if low_rating_positive > 0:
                    report_lines.append(f"   - {low_rating_positive} low-rating reviews classified as positive")
                if high_rating_negative > 0:
                    report_lines.append(f"   - {high_rating_negative} high-rating reviews classified as negative")
            else:
                report_lines.append("âœ… Sentiment analysis shows good consistency with ratings")

            # äº§å“è¯„ä»·æ€»ç»“
            report_lines.append("")
            if avg_rating >= 4.5 and pos_ratio > 70:
                report_lines.append(
                    "ğŸŒŸ Excellent Product Performance: High ratings with predominantly positive sentiment")
            elif avg_rating >= 4.0 and pos_ratio > 60:
                report_lines.append("âœ… Good Product Performance: Solid ratings with mostly positive feedback")
            elif avg_rating >= 3.5 and neu_ratio > 30:
                report_lines.append("âš¡ Moderate Product Performance: Mixed feedback with significant neutral sentiment")
            elif avg_rating < 3.0 or neg_ratio > 40:
                report_lines.append("âš ï¸ Product Needs Improvement: Low ratings or high negative sentiment detected")
            else:
                report_lines.append("ğŸ“Š Average Product Performance: Balanced feedback across sentiments")

        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        with open('analysis_results/analysis_report.txt', 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))

        print("âœ… æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜")

        # ä¿å­˜è¯¦ç»†æ•°æ®
        df.to_csv('analysis_results/detailed_results.csv', index=False, encoding='utf-8-sig')
        print("âœ… è¯¦ç»†æ•°æ®å·²ä¿å­˜")

        # ç”Ÿæˆé«˜é¢‘è¯ç»Ÿè®¡è¡¨ï¼ˆé¢å¤–çš„CSVæ–‡ä»¶ï¼‰
        word_stats = []
        for sentiment in ['positive', 'negative', 'neutral']:
            sentiment_reviews = df[df['sentiment'] == sentiment]['full_content'].astype(str)
            if len(sentiment_reviews) > 0:
                sentiment_text = ' '.join(sentiment_reviews)
                sentiment_words = re.findall(r'\b[a-zA-Z]{3,}\b', sentiment_text.lower())
                sentiment_filtered = [w for w in sentiment_words if w not in stop_words]
                sentiment_freq = Counter(sentiment_filtered).most_common(20)

                for word, count in sentiment_freq:
                    word_stats.append({
                        'sentiment': sentiment,
                        'word': word,
                        'frequency': count,
                        'percentage': count / len(sentiment_filtered) * 100 if sentiment_filtered else 0
                    })

        if word_stats:
            word_df = pd.DataFrame(word_stats)
            word_df.to_csv('analysis_results/word_frequency_analysis.csv', index=False, encoding='utf-8-sig')
            print("âœ… è¯é¢‘åˆ†æè¡¨å·²ä¿å­˜")

        self.create_visualizations(df, sentiment_counts)
        print("\nğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ° 'analysis_results' æ–‡ä»¶å¤¹")

        # æ‰“å°æŠ¥å‘Šæ‘˜è¦
        print("\n" + "=" * 60)
        print("Report Summary")
        print("=" * 60)
        for line in report_lines[:50]:  # æ‰“å°å‰50è¡Œä½œä¸ºæ‘˜è¦
            print(line)

    def create_visualizations(self, df, sentiment_counts):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨ - å¢å¼ºç‰ˆwithé«˜é¢‘è¯å’Œæ˜Ÿçº§ç»Ÿè®¡"""
        print("ğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

        sns.set_style("whitegrid")
        colors = {'positive': '#2ecc71', 'negative': '#e74c3c', 'neutral': '#95a5a6'}

        # åœç”¨è¯åˆ—è¡¨
        stop_words = {
            'the', 'and', 'for', 'with', 'this', 'that', 'have', 'from',
            'will', 'your', 'more', 'been', 'what', 'were', 'there', 'their',
            'would', 'could', 'very', 'been', 'have', 'also', 'just', 'only',
            'other', 'after', 'before', 'some', 'when', 'which', 'where',
            'these', 'those', 'then', 'than', 'both', 'each', 'they', 'them',
            'was', 'are', 'has', 'had', 'but', 'not', 'can', 'did', 'does'
        }

        # 1. æƒ…æ„Ÿåˆ†å¸ƒé¥¼å›¾å’Œè¯„åˆ†åˆ†å¸ƒ
        fig1, axes = plt.subplots(1, 2, figsize=(14, 6))

        # é¥¼å›¾
        ax1 = axes[0]
        plot_colors = [colors.get(s, '#333') for s in sentiment_counts.index]
        wedges, texts, autotexts = ax1.pie(
            sentiment_counts.values,
            labels=sentiment_counts.index,
            colors=plot_colors,
            autopct='%1.1f%%',
            startangle=90,
            pctdistance=0.85
        )

        # è°ƒæ•´æ ‡ç­¾ä½ç½®é¿å…é‡å 
        for text in texts:
            text.set_fontsize(10)
        for autotext in autotexts:
            autotext.set_color('white')
            autotext.set_fontsize(9)
            autotext.set_weight('bold')

        ax1.set_title('Sentiment Distribution', fontsize=12, fontweight='bold', pad=20)

        # 2. è¯„åˆ†åˆ†å¸ƒ
        if 'rating' in df.columns and df['rating'].notna().any():
            ax2 = axes[1]
            rating_counts = df['rating'].value_counts().sort_index()
            bars = ax2.bar(rating_counts.index, rating_counts.values,
                           color=['#e74c3c', '#f39c12', '#f1c40f', '#3498db', '#2ecc71'],
                           width=0.6)
            ax2.set_xlabel('Rating (Stars)', fontsize=10)
            ax2.set_ylabel('Count', fontsize=10)
            ax2.set_title('Rating Distribution', fontsize=12, fontweight='bold', pad=20)
            ax2.set_xticks([1, 2, 3, 4, 5])

            # æ·»åŠ æ•°å€¼æ ‡ç­¾ï¼Œè°ƒæ•´ä½ç½®
            for bar in bars:
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width() / 2., height + rating_counts.max() * 0.01,
                         f'{int(height)}', ha='center', va='bottom', fontsize=9)

            ax2.set_ylim(0, rating_counts.max() * 1.1)  # ç•™å‡ºç©ºé—´ç»™æ ‡ç­¾

        plt.tight_layout()
        plt.savefig('analysis_results/distributions.png', dpi=150, bbox_inches='tight')
        plt.close(fig1)

        # 3. æƒ…æ„Ÿä¸è¯„åˆ†çš„å…³ç³»å›¾
        if 'rating' in df.columns and df['rating'].notna().any():
            fig2, axes = plt.subplots(1, 2, figsize=(14, 6))

            # å †å æŸ±çŠ¶å›¾
            ax1 = axes[0]
            rating_sentiment = pd.crosstab(df['rating'], df['sentiment'])
            rating_sentiment.plot(kind='bar', stacked=True,
                                  color=[colors.get(col, '#333') for col in rating_sentiment.columns],
                                  ax=ax1, width=0.7)
            ax1.set_xlabel('Rating (Stars)', fontsize=10)
            ax1.set_ylabel('Count', fontsize=10)
            ax1.set_title('Sentiment by Rating', fontsize=12, fontweight='bold', pad=20)
            ax1.legend(title='Sentiment', loc='upper left', fontsize=9, framealpha=0.9)
            ax1.set_xticklabels(ax1.get_xticklabels(), rotation=0)

            # å¹³å‡è¯„åˆ†å›¾
            ax2 = axes[1]
            sentiment_rating = df.groupby('sentiment')['rating'].mean().sort_values()
            bars = ax2.bar(range(len(sentiment_rating)), sentiment_rating.values,
                           color=[colors.get(s, '#333') for s in sentiment_rating.index],
                           width=0.6)
            ax2.set_xlabel('Sentiment', fontsize=10)
            ax2.set_ylabel('Average Rating', fontsize=10)
            ax2.set_title('Average Rating by Sentiment', fontsize=12, fontweight='bold', pad=20)
            ax2.set_ylim(0, 5.5)
            ax2.set_xticks(range(len(sentiment_rating)))
            ax2.set_xticklabels(sentiment_rating.index)

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, bar in enumerate(bars):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width() / 2., height + 0.1,
                         f'{height:.2f}', ha='center', va='bottom', fontsize=10)

            plt.tight_layout()
            plt.savefig('analysis_results/sentiment_rating_analysis.png', dpi=150, bbox_inches='tight')
            plt.close(fig2)

        # 4. è¯äº‘
        fig3, axes = plt.subplots(1, 2, figsize=(14, 6))

        # æ­£é¢è¯äº‘
        ax1 = axes[0]
        positive_text = ' '.join(df[df['sentiment'] == 'positive']['full_content'].astype(str))
        if positive_text:
            try:
                wordcloud_pos = WordCloud(
                    width=600, height=400,
                    background_color='white',
                    colormap='Greens',
                    max_words=30,
                    relative_scaling=0.5,
                    min_font_size=10
                ).generate(positive_text)
                ax1.imshow(wordcloud_pos, interpolation='bilinear')
                ax1.axis('off')
                ax1.set_title('Positive Reviews Word Cloud', fontsize=12, fontweight='bold', y=1.02)
            except:
                ax1.text(0.5, 0.5, 'Word cloud generation failed', ha='center', va='center')
                ax1.axis('off')

        # è´Ÿé¢è¯äº‘
        ax2 = axes[1]
        negative_text = ' '.join(df[df['sentiment'] == 'negative']['full_content'].astype(str))
        if negative_text:
            try:
                wordcloud_neg = WordCloud(
                    width=600, height=400,
                    background_color='white',
                    colormap='Reds',
                    max_words=30,
                    relative_scaling=0.5,
                    min_font_size=10
                ).generate(negative_text)
                ax2.imshow(wordcloud_neg, interpolation='bilinear')
                ax2.axis('off')
                ax2.set_title('Negative Reviews Word Cloud', fontsize=12, fontweight='bold', y=1.02)
            except:
                ax2.text(0.5, 0.5, 'Word cloud generation failed', ha='center', va='center')
                ax2.axis('off')

        plt.tight_layout()
        plt.savefig('analysis_results/word_clouds.png', dpi=150, bbox_inches='tight')
        plt.close(fig3)

        # 5. æ–°å¢ï¼šé«˜é¢‘è¯å¯¹æ¯”å›¾
        fig4, axes = plt.subplots(2, 2, figsize=(16, 10))

        all_text = ' '.join(df['full_content'].astype(str))
        all_words = re.findall(r'\b[a-zA-Z]{3,}\b', all_text.lower())
        all_filtered = [w for w in all_words if w not in stop_words]

        # å­å›¾1ï¼šæ•´ä½“é«˜é¢‘è¯
        ax1 = axes[0, 0]
        overall_freq = Counter(all_filtered).most_common(10)
        if overall_freq:
            words, counts = zip(*overall_freq)
            y_pos = np.arange(len(words))
            ax1.barh(y_pos, counts, color='#3498db', alpha=0.8)
            ax1.set_yticks(y_pos)
            ax1.set_yticklabels(words, fontsize=9)
            ax1.set_xlabel('Frequency', fontsize=10)
            ax1.set_title('Top 10 Overall High Frequency Words', fontsize=11, fontweight='bold')
            ax1.invert_yaxis()

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for i, v in enumerate(counts):
                ax1.text(v + max(counts) * 0.01, i, str(v), va='center', fontsize=8)

        # å­å›¾2ï¼šå„æƒ…æ„Ÿç±»åˆ«é«˜é¢‘è¯å¯¹æ¯”
        ax2 = axes[0, 1]
        sentiment_word_data = []
        for sentiment in ['positive', 'negative', 'neutral']:
            sentiment_reviews = df[df['sentiment'] == sentiment]['full_content'].astype(str)
            if len(sentiment_reviews) > 0:
                sentiment_text = ' '.join(sentiment_reviews)
                sentiment_words = re.findall(r'\b[a-zA-Z]{3,}\b', sentiment_text.lower())
                sentiment_filtered = [w for w in sentiment_words if w not in stop_words]
                sentiment_freq = Counter(sentiment_filtered).most_common(5)
                for word, count in sentiment_freq:
                    sentiment_word_data.append({
                        'word': word,
                        'sentiment': sentiment,
                        'count': count
                    })

        if sentiment_word_data:
            word_df = pd.DataFrame(sentiment_word_data)
            word_pivot = word_df.pivot_table(index='word', columns='sentiment', values='count', fill_value=0)
            word_pivot.plot(kind='barh', stacked=False, ax=ax2,
                            color=[colors.get(col, '#333') for col in word_pivot.columns])
            ax2.set_xlabel('Frequency', fontsize=10)
            ax2.set_title('Top Words Comparison by Sentiment', fontsize=11, fontweight='bold')
            ax2.legend(title='Sentiment', loc='best', fontsize=8)

        # å­å›¾3ï¼šæ˜Ÿçº§åˆ†å¸ƒè¯¦ç»†ç»Ÿè®¡
        if 'rating' in df.columns and df['rating'].notna().any():
            ax3 = axes[1, 0]

            # åˆ›å»ºæ˜Ÿçº§ä¸æƒ…æ„Ÿçš„çƒ­åŠ›å›¾æ•°æ®
            rating_sentiment_matrix = pd.crosstab(df['rating'], df['sentiment'], normalize='index') * 100

            # ç»˜åˆ¶çƒ­åŠ›å›¾
            im = ax3.imshow(rating_sentiment_matrix.values, cmap='YlOrRd', aspect='auto')

            # è®¾ç½®æ ‡ç­¾
            ax3.set_xticks(np.arange(len(rating_sentiment_matrix.columns)))
            ax3.set_yticks(np.arange(len(rating_sentiment_matrix.index)))
            ax3.set_xticklabels(rating_sentiment_matrix.columns)
            ax3.set_yticklabels([f'{int(r)}â˜…' for r in rating_sentiment_matrix.index])
            ax3.set_xlabel('Sentiment', fontsize=10)
            ax3.set_ylabel('Rating', fontsize=10)
            ax3.set_title('Rating-Sentiment Heatmap (%)', fontsize=11, fontweight='bold')

            # æ·»åŠ æ•°å€¼
            for i in range(len(rating_sentiment_matrix.index)):
                for j in range(len(rating_sentiment_matrix.columns)):
                    text = ax3.text(j, i, f'{rating_sentiment_matrix.values[i, j]:.1f}',
                                    ha="center", va="center", color="black", fontsize=9)

            # æ·»åŠ é¢œè‰²æ¡
            plt.colorbar(im, ax=ax3, fraction=0.046, pad=0.04)

        # å­å›¾4ï¼šå…³é”®çŸ­è¯­é¢‘ç‡
        ax4 = axes[1, 1]

        def extract_phrases(text, n=2):
            words = re.findall(r'\b[a-zA-Z]+\b', text.lower())
            phrases = []
            for i in range(len(words) - n + 1):
                phrase_words = words[i:i + n]
                if not any(w in stop_words for w in phrase_words):
                    phrases.append(' '.join(phrase_words))
            return phrases

        all_phrases = extract_phrases(all_text, 2)
        phrase_freq = Counter(all_phrases).most_common(10)

        if phrase_freq:
            phrases, counts = zip(*phrase_freq)
            # åªæ˜¾ç¤ºå‡ºç°3æ¬¡ä»¥ä¸Šçš„çŸ­è¯­
            filtered_phrases = [(p, c) for p, c in zip(phrases, counts) if c > 2]

            if filtered_phrases:
                phrases, counts = zip(*filtered_phrases)
                y_pos = np.arange(len(phrases))
                ax4.barh(y_pos, counts, color='#9b59b6', alpha=0.8)
                ax4.set_yticks(y_pos)
                ax4.set_yticklabels(phrases, fontsize=8)
                ax4.set_xlabel('Frequency', fontsize=10)
                ax4.set_title('Top Key Phrases (2-word combinations)', fontsize=11, fontweight='bold')
                ax4.invert_yaxis()

                # æ·»åŠ æ•°å€¼æ ‡ç­¾
                for i, v in enumerate(counts):
                    ax4.text(v + max(counts) * 0.01, i, str(v), va='center', fontsize=8)

        plt.tight_layout()
        plt.savefig('analysis_results/advanced_analysis.png', dpi=150, bbox_inches='tight')
        plt.close(fig4)

        print("âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜")


def main():
    """ä¸»ç¨‹åº"""
    print("=" * 60)
    print(" " * 15 + "Amazon Review Sentiment Analysis System")
    print("=" * 60)

    use_api = USE_API
    api_key = DEEPSEEK_API_KEY
    input_source = "amazon_reviews.csv"

    if not os.path.exists(input_source):
        print(f"\nâŒ æ–‡ä»¶ '{input_source}' ä¸å­˜åœ¨")
        return

    analyzer = AmazonReviewAnalyzer(use_api=use_api, api_key=api_key)

    print(f"\nğŸš€ å¼€å§‹åˆ†æ: {input_source}")
    df = analyzer.analyze_reviews(input_source)

    if df is not None and len(df) > 0:
        analyzer.generate_report(df)

        print("\n" + "=" * 60)
        print("ğŸ‰ åˆ†æå®Œæˆ!")
        print("=" * 60)

        sentiment_counts = df['sentiment'].value_counts()
        total = len(df)

        print("\nğŸ“Š åˆ†ææ‘˜è¦:")
        print("-" * 40)

        for sentiment in ['positive', 'negative', 'neutral']:
            if sentiment in sentiment_counts.index:
                count = sentiment_counts[sentiment]
                percentage = count / total * 100
                emoji = {'positive': 'ğŸ˜Š', 'negative': 'ğŸ˜”', 'neutral': 'ğŸ˜'}[sentiment]
                print(f"  {emoji} {sentiment.capitalize():8s}: {count:4d} æ¡ ({percentage:6.2f}%)")

        if 'rating' in df.columns and df['rating'].notna().any():
            print(f"\n  â­ å¹³å‡è¯„åˆ†: {df['rating'].mean():.2f}/5.0")

        print("\nğŸ“ è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹ 'analysis_results' æ–‡ä»¶å¤¹")
    else:
        print("\nâŒ åˆ†æå¤±è´¥")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºé”™è¯¯: {e}")
        import traceback

        traceback.print_exc()

